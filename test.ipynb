{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced9373e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, DetectorFactory, detect_langs\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# Set seed for consistent language detection results\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "class Translation:\n",
    "    def __init__(self, text, destination):\n",
    "        self.text = text\n",
    "        self.destination = destination\n",
    "        try:\n",
    "            detected_languages = detect_langs(self.text)\n",
    "            print(f\"Detected languages with confidence: {detected_languages}\")  # Debugging detected languages\n",
    "            # Use the language with the highest confidence\n",
    "            self.original = detected_languages[0].lang\n",
    "            confidence = detected_languages[0].prob\n",
    "            print(f\"Detected language: {self.original} with confidence: {confidence}\")\n",
    "            # Fallback to English if confidence is below 0.8\n",
    "            if confidence < 0.8:\n",
    "                print(\"Low confidence in detected language. Forcing language to English.\")\n",
    "                self.original = \"en\"\n",
    "        except Exception as e:\n",
    "            print(f\"Error detecting language: {e}\")\n",
    "            self.original = \"en\"  # Default to English on error\n",
    "    def translatef(self):\n",
    "        try:\n",
    "            if self.original != self.destination:\n",
    "                print(f\"Translating from {self.original} to {self.destination}: {self.text}\")  # Debugging translation\n",
    "                translator = GoogleTranslator(source=self.original, target=self.destination)\n",
    "                translation = translator.translate(self.text)\n",
    "                print(f\"Translation result: {translation}\")  # Debugging translation result\n",
    "                return translation\n",
    "            return self.text  # No translation needed if languages match\n",
    "        except Exception as e:\n",
    "            print(f\"Error during translation: {e}\")\n",
    "            return self.text  # Return original text if translation fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c45b5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\chat_interface.py:338: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n",
      "ERROR:    [Errno 10048] error while attempting to bind on address ('0.0.0.0', 7860): only one usage of each socket address (protocol/network address/port) is normally permitted\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original language (after fallback if needed): vi\n",
      "Determined max_new_tokens: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.4` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated response: Please provide a concise and relevant response to: Hello, I am interested in your product. Please send me more information about it.\n",
      "Hello! My name\n",
      "Translated response: Vui lòng cung cấp một phản hồi ngắn gọn và có liên quan đến: Xin chào, tôi quan tâm đến sản phẩm của bạn. Vui lòng gửi cho tôi thêm thông tin về nó.\n",
      "Xin chào! Tên tôi\n",
      "Using cached response.\n",
      "Original language (after fallback if needed): fi\n",
      "Determined max_new_tokens: 20\n",
      "Generated response: Please provide a concise and relevant response to: Hello, I am interested in your product. Please send me more information about it.\n",
      "Hello! My name\n",
      "Translated response: Anna tiivis ja asiaankuuluva vastaus: Hei, olen kiinnostunut tuotteestasi. Lähetä minulle lisätietoja siitä.\n",
      "Hei! Minun nimeni\n",
      "Original language (after fallback if needed): sw\n",
      "Determined max_new_tokens: 20\n",
      "Generated response: Please provide a concise and relevant response to: Hi, I am interested in your product. Please send me more information about it.\n",
      "Hi there! My\n",
      "Translated response: Tafadhali toa majibu mafupi na yanayofaa kwa: hi, ninavutiwa na bidhaa yako. Tafadhali nitumie habari zaidi juu yake.\n",
      "Halo hapo! Yangu\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from load_model import pipe\n",
    "\n",
    "# Add caching for translations and responses\n",
    "translation_cache = {}\n",
    "response_cache = {}\n",
    "\n",
    "def is_greeting(message):\n",
    "    greetings = [\"hello\", \"hi\", \"hey\"]\n",
    "    return message.lower() in greetings\n",
    "\n",
    "def determine_response_length(message):\n",
    "    # Short responses for greetings or explicit brevity requests\n",
    "    if is_greeting(message) or \"in 10 words\" in message.lower():\n",
    "        return 20  # Very short response\n",
    "    return 100  # Longer response for detailed questions\n",
    "\n",
    "def reply(message, history):\n",
    "    try:\n",
    "        if message in response_cache:\n",
    "            print(\"Using cached response.\")  # Debugging\n",
    "            return response_cache[message]\n",
    "\n",
    "        # Detect and translate input if needed\n",
    "        txt = Translation(message, \"en\")\n",
    "        print(f\"Original language (after fallback if needed): {txt.original}\")  # Debugging original language\n",
    "        if txt.original != \"en\":\n",
    "            if message in translation_cache:\n",
    "                translation = translation_cache[message]\n",
    "            else:\n",
    "                translation = txt.translatef()\n",
    "                translation_cache[message] = translation\n",
    "        else:\n",
    "            translation = message\n",
    "\n",
    "        max_tokens = determine_response_length(translation)\n",
    "        print(f\"Determined max_new_tokens: {max_tokens}\")  # Debugging response length\n",
    "\n",
    "        # Generate response\n",
    "        guided_message = f\"Please provide a concise and relevant response to: {translation}\"\n",
    "        response = pipe(guided_message, max_new_tokens=max_tokens)\n",
    "        result = response[0][\"generated_text\"]\n",
    "        print(f\"Generated response: {result}\")  # Debugging generated response\n",
    "\n",
    "        # Translate response back to original language if needed\n",
    "        if txt.original != \"en\":\n",
    "            t = Translation(result, txt.original)\n",
    "            result = t.translatef()\n",
    "            print(f\"Translated response: {result}\")  # Debugging translated response\n",
    "\n",
    "        response_cache[message] = result\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error in reply function: {e}\")\n",
    "        return \"An error occurred. Please try again.\"\n",
    "\n",
    "demo = gr.ChatInterface(fn=reply, title=\"Multilingual-Bloom Bot\")\n",
    "demo.launch(server_name=\"0.0.0.0\", share=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
